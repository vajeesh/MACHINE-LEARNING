{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employee Performance Analysis\n",
    "#### FicZon. \n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <i>Project Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Business case for PRCL-0019 - Project</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data science project which is given here is an FicZon sales \n",
    "performance. The goal of the project is to find the Sales status of the company depending on features of the data, Such as Created, Product_ID, Source, Mobile, EMAIL, Sales_Agent, Location, Delivery_Mode, Status etc.,<br> The Goal and Insights of the project as follows:\n",
    "1. Department wise performances\n",
    "* Top 3 Important Factors effecting status\n",
    "* A trained model which can predict the status based on factors as inputs.<br>\n",
    "This will be used to hire employees\n",
    "* Recommendations to improve the sales based on insights from analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given dataset consist of 7422 rows. The features present \n",
    "in the data are 28 columns. The shape of the dataset is 7422x9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Correlation we can get the important aspects of the data, Correlation between features and Status.Correlation is a statistical measure <br> that expresses the extent to which two variables are linearly related.The analysis of the project has gone through the stage of distribution analysis,<br> correlation analysis and analysis by each department to satisfy the project goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of Categorical data and Numerical data. The Machine Learning model which works well for categorical data is XGBoost.  Target variable consist of ordinal data, so this is  a classification problem.The machine learning model which is used in this project is random forest classifier which predicts higher accuracy 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the important goal of this project is to find the important feature affecting the sales. The important features were predicted using the machine learning model feature importance technique. The main technique used in the preprocessing data using the Label Encoding method to convert the string - categorical data into numerical data, because, Most of machine learning methods are based on numerical methods where strings are not supportive. The overall project was performed and achieved the goals by using the machine learning model and visualization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Normal Distribution\n",
    "* Checking weather the data is Normally distributed or Not with Skewness and Kurtosis, By defining a funtion\n",
    "* Status, This column is skewed\n",
    "* Range of skewness & kurtosis, S< |1.96|\n",
    "* skewness for Status: 0.02\n",
    "* kurtosis for Status: 1.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Skewed Data for Machine Learning\n",
    "* Skewed data is common in data science: skew is the degree of distortion from a normal distribution.<br>\n",
    "\n",
    "### Square Root Transformation\n",
    "* Square root transformation is one of the many types of standard transformations.This transformation is used for count data (data that follow a Poisson distribution) or small whole numbers. Each data point is replaced by its square root. Negative data is converted to positive by adding a constant, and then transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "The Data cleaning and wrangling is the part of the Data science project where the workflow the project go through this stage. because the damaged and missing data will lead to the disaster in the accuracy and quality of the model. If the data is already structured and cleaned, there is no need for the data cleaning. In this case, the given data is well structured and cleaned and there are no missing data present in this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by Visualization\n",
    "we can able to perform the analysis by the visualisation of the data in two forms here in this project. One is by distributing the data and visualize using the density plotting. The other one is nothing but the correlation method which will visualize the correlation heat map and we can able to achieve the correlation values between the numerical features.\n",
    "### 1. Distribution Plot\n",
    "In general, one of the first few steps in exploring the data would be to have a rough idea of how the features are distributed with one another. To do so, we shall invoke the familiar kdeplot function from the Seaborn plotting library. The distribution has been done by both numerical and categorical features. it will show the overall idea about the density and majority of data present in a different level.\n",
    "### 2. Correlation\n",
    "The next tool is Heatmep with correlation matrix. By plotting a correlation matrix, we have a very nice overview of how the features are related to one another. For a Pandas data frame, we can conveniently use .corr which by default provides the Pearson Correlation values of the columns pairwise in that data frame. The correlation works best for numerical data where we are going to use all the numerical features present in the data.\n",
    "\n",
    "From the above Pearson correlation heat plot, we can see that correlation between features with numerical values in the dataset. The heat signatures show the level of correlation from 0 to 1. from this distribution we can derive the facts as follows:\n",
    "\n",
    "* The Sales Agent and Month are having the higher correlation when comparing to all features.\n",
    "* Delivery mode company and years with the Status has the negative relation between these features.\n",
    "* Location company and years with the Status has the negative relation between these features.\n",
    "\n",
    "# Machine Learning Model\n",
    "\n",
    "By performing 20 different models XGBoost Classifier and Extra tree classifier performs best for this data set\n",
    "\n",
    "The machine learning models used in this project are\n",
    "\n",
    "1.<b>XGBoost classifier</b>\n",
    "\n",
    "2.<b>Random Forest classifier</b>\n",
    "\n",
    "3.<b>Extratree Classifier</b>\n",
    "\n",
    "Both machine learning algorithms are best for classification and labelled data. The train and test data are divided and fitted into the model and passed through the machine learning. Since we have already noted the severe imbalance in the values within the target variable, we implement the SMOTE method in the dealing with this skewed value via the learn Python package. The predicted data and test data achieved the accuracy rate of,\n",
    "\n",
    "* <b>XGBoost classifier</b>: 75% accuracy\n",
    "* <b>Random Forest classifier</b>: 60% accuracy\n",
    "* <b>Extratree Classifier</b>: 75% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <i>3. Summary</i>\n",
    "The machine learning model has been fitted and predicted with the accuracy score. The goal of this project is nothing but the results from the analysis and machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Top 3 Important Factors effecting employee performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Feature Selection Technique in sklearn also contains a very convenient and most useful attribute feature importance which tells us which features in our dataset has given most importance through ML. From Heatmap also we get important features. Both Techqiues are giving same results.\n",
    "The top three important features affecting the performance rating are ordered with their importance level as follows,\n",
    "\n",
    "1. Location\n",
    "2. Sales Agent \n",
    "3. Delivery mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 2: A Trained model which can predict the employee performance\n",
    "The trained model is created using the machine learning algorithm as follows with the accuracy score,\n",
    "\n",
    "* <b>XGBoost classifier</b>: 75% accuracy\n",
    "* <b>Random Forest classifier</b>: 60% accuracy\n",
    "* <b>Extratree Classifier</b>: 75% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 3: Recommendations to improve the employee performance\n",
    "\n",
    "* Keep trials short\n",
    "* Optimize your email campaign\n",
    "* Call your trial signups immediately\n",
    "* Give short, value-focused demos\n",
    "* Follow up relentlessly\n",
    "* Set your prices (really) high\n",
    "* Sell prepaid annual plans\n",
    "* Donâ€™t give discounts\n",
    "* Never close a bad deal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
